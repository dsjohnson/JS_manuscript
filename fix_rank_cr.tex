\documentclass[12pt]{article}

\usepackage[margin=1in]{geometry}

\usepackage{amsmath,amssymb,amsthm,epsfig,rotfloat,psfrag,natbib,url,graphicx,lineno,hyperref}
\usepackage{authblk}

\usepackage[symbol]{footmisc}

\usepackage{blkarray}

\usepackage{booktabs} 
\usepackage{makecell}

\usepackage{algorithm, algorithmic}



\bibliographystyle{apalike}

%%%%
% Some shortcut notation
%%%%

\newcommand{\bu}{\ensuremath{\mathbf{u}}}
\newcommand{\bx}{\ensuremath{\mathbf{x}}}
\newcommand{\by}{\ensuremath{\mathbf{y}}}
\newcommand{\bB}{\ensuremath{\mathbf{B}}}
\newcommand{\bp}{\ensuremath{\mathbf{p}}}
\newcommand{\bc}{\ensuremath{\mathbf{c}}}
\newcommand{\bfr}{\ensuremath{\mathbf{f}}}
\newcommand{\bn}{\ensuremath{\mathbf{n}}}
\newcommand{\bP}{\ensuremath{\mathbf{P}}}
\newcommand{\bz}{\ensuremath{\mathbf{z}}}
\newcommand{\bN}{\ensuremath{\mathbf{N}}}
\newcommand{\bT}{\ensuremath{\mathbf{T}}}

\newcommand{\bxi}{\ensuremath{\boldsymbol{\xi}}}
\newcommand{\bPsi}{\ensuremath{\boldsymbol{\Psi}}}
\newcommand{\bphi}{\ensuremath{\boldsymbol{\phi}}}
\newcommand{\bpi}{\ensuremath{\boldsymbol{\pi}}}
\newcommand{\bG}{\ensuremath{\boldsymbol{\Gamma}}}
\newcommand{\bt}{\ensuremath{\boldsymbol{\theta}}}
\newcommand{\bvt}{\ensuremath{\boldsymbol{\vartheta}}}

\newcommand{\jags}{{\tt JAGS }}
\newcommand{\nimble}{{\tt nimble }}

%%%%%%%%%%%%%


\newcommand{\bX}{\ensuremath{\mathbf{X}}}
\newcommand{\bC}{\ensuremath{\mathbf{C}}}
\newcommand{\bW}{\ensuremath{\mathbf{W}}}
\newcommand{\bK}{\ensuremath{\mathbf{K}}}
\newcommand{\bk}{\ensuremath{\mathbf{k}}}
\newcommand{\bI}{\ensuremath{\mathbf{I}}}
\newcommand{\bH}{\ensuremath{\mathbf{H}}}
\newcommand{\bh}{\ensuremath{\mathbf{h}}}
\newcommand{\bw}{\ensuremath{\mathbf{w}}}
\newcommand{\bD}{\ensuremath{\mathbf{D}}}
\newcommand{\bdp}{\ensuremath{\mathbf{d}^+}}



\newcommand{\bb}{\ensuremath{\boldsymbol{\beta}}}
\newcommand{\ba}{\ensuremath{\boldsymbol{\alpha}}}
\newcommand{\be}{\ensuremath{\boldsymbol{\epsilon}}}
\newcommand{\bSig}{\ensuremath{\boldsymbol{\Sigma}}}
\newcommand{\bmu}{\ensuremath{\boldsymbol{\mu}}}
\newcommand{\bO}{\ensuremath{\boldsymbol{\Omega}}}
\newcommand{\bs}{\ensuremath{\boldsymbol{\sigma}}}
\newcommand{\bo}{\ensuremath{\boldsymbol{\omega}}}
\newcommand{\bg}{\ensuremath{\boldsymbol{\gamma}}}



%mathcal
\newcommand{\fN}{\ensuremath{\mathcal{N}}}
\newcommand{\fG}{\ensuremath{\mathcal{G}}}
\newcommand{\fP}{\ensuremath{\mathcal{P}}}
\newcommand{\fH}{\ensuremath{\mathcal{H}}}

\raggedright
\raggedbottom

\begin{document}
%\vspace*{0.15\textheight}

\vspace*{1in}

%\begin{center}
\setlength{\parindent}{0pt}
\renewcommand{\baselinestretch}{1.8}\normalsize

\begin{center}
\noindent\hrulefill

{\Large Efficient Bayesian Estimation for Open Population Capture-Recapture Models Without Data Augmentation} 

\noindent\hrulefill

\end{center}

\renewcommand{\baselinestretch}{1.15}\normalsize 
\bigskip

\begin{center}
{\bf Devin S. Johnson}\\
 Pacific Islands Fisheries Science Center,\\
 National Marine Fisheries Service, NOAA, \\
 Honolulu, Hawai`i, USA \\
 email: {\tt devin.johnson@noaa.gov} \\ \bigskip

{\bf Janelle J. Badger} \\
 Pacific Islands Fisheries Science Center,\\
 National Marine Fisheries Service, NOAA, \\
 Honolulu, Hawai`i, USA \\ \bigskip

{\bf Shelbie Ishimaru} \\
Hawai`i Institute of Marine Biology, \\
University of Hawai`i at Mānoa, \\
Honolulu, Hawai`i, USA\\ 
and \\
 Pacific Islands Fisheries Science Center,\\
 National Marine Fisheries Service, NOAA, \\
 Honolulu, Hawai`i, USA \\ \bigskip


\bigskip

\today

\end{center}

\vspace*{\fill}

\clearpage




%\renewcommand{\baselinestretch}{1.15}\normalsize
\linenumbers

%% ABSTRACT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\vspace*{0.25\textheight}
\begin{center}
\begin{minipage}{0.65\paperwidth}
\renewcommand{\baselinestretch}{1}\normalsize

\centerline{\bf Abstract} 
This happens last. \bigskip

{\bf Key words}: Some, Key, words, Go, Here
\end{minipage}
\end{center}

\clearpage


\renewcommand{\baselinestretch}{1.25}\normalsize
\raggedright
\setlength{\parindent}{2em}
\raggedbottom
\linenumbers


\section{Introduction}

For 60 years, open-population capture–recapture models, particularly those in the Jolly–Seber (JS) family \citep{Jolly1965,Seber1965}, have been fundamental tools for estimating demographic rates such as survival, recruitment, and population size in open systems where individuals can enter or leave the population between sampling occasions. Classical maximum-likelihood estimators and their extensions \citep{schwarz1996general} have long provided practical means for inference, yet they are often limited in their capacity to accommodate hierarchical structures, covariate effects, or complex dependence among parameters. Bayesian approaches have addressed many of these limitations, enabling flexible modeling and coherent quantification of uncertainty \citep{Royle:2008kx}. 

Adoption of Bayesian inference for JS models was initially hindered in regular use because standard Bayesian Markov Chain Monte Carlo (MCMC) software at the time, such as {\tt WinBUGS} \citep{LunnEtAl2000a} or {\tt JAGS} \citep{plummer2003jags} did not have the capability for calculating the JS likelihood within the MCMC. Therefore, practitioners would have to code the JS likelihood, as well as, bespoke MCMC algorithms for Bayesian estimation of JS parameters. The ground breaking papers of \cite{Durban:2005yw} and \cite{Royle:2007kx} introduced the concept of a population of augmented individuals which may or may not be part of the true population of interest, depending on whether or not they entered based on an probabilistic binary indicator of inclusion. This parameter expansion data augmentation (PX-DA) specification was formalized for JS models by \cite{RoyleDorazio2012a}. By introducing latent inclusion indicators as part of the hierarchical model, PX-DA allows MCMC sampling in a fixed-dimensional parameter space, where abundance is simply the sum of the latent inclusion indicators and not a model parameter. This breakthrough in JS specification allowed users to construct a JS model hierarchically in {\tt WinBUGS} or {\tt JAGS} using simple, standard distributions. Due to the ease in which JS models could be fit with MCMC methods, regular use of Bayesian methods for fitting JS models increased dramatically and augmentation methods became the dominant paradigm for abundance estimation using JS models in an MCMC setting. The PX-DA approach opened many doors for fitting models that cannot be easily fit with maximum likelihood methods, namely those including individual heterogeneity components (e.g., \citealt{Royle:2008kx}, Ch. 10) and other hierarchical specifications for time-indexed effects. 

Although data augmentation was instrumental in ushering in a new era a Bayesian JS modeling, the convenience comes at a cost: the approach can be computationally inefficient for large augmentation data sets \citep{Royle:2007kx} and prone to slow mixing \citep{SchofieldBarker2008a}. For example, if the probability of detection is relatively low and the population is large, many augmented individuals need to be added the the data and all states for all individuals must be updated even when their current inclusion indicator does not indicate they are part of the actual population. In addition, \cite{link2013cautionary} notes some questionable properties of PX-DA method relative to the discrete uniform prior induced by the parameter expansion. To alleviate some of these issues and speed computation \cite{yackulic2020need} and \cite{saracco2023fast} proposed the ``marginalized'' capture-recpature models, which formulates capture-recapture models as a Hidden Markov Models (HMM; \citealt{zucchini2016hidden}). The HMM forward algorithm allows one to marginalize over latent states so that they need not be sampled as part of the MCMC. This increases speed and often improves MCMC chain mixing. However, the user still augments data for undetected individuals and uses the MCMC to marginalize over those not included in the population. In this paper, we introduce a fully marginalized Bayesian formulation of the JS model that eliminates the need for any data augmentation. Our approach integrates analytically over both the unobserved individual encounter histories and the latent states of all individuals using a conditional HMM formulation of JS family models. In essence we propose using the exact JS likelihood within a {\it collapsed} Gibbs sampler \citep{Liu1994a} to estimate parameters and derived quantities of JS models. 

Collapsed Gibbs samplers offer a method to improve MCMC convergence over standard Gibbs samplers. \cite{Liu1994a} provides a mathematical proof which shows that the rate of convergence for a collapsed Gibbs sampler is at least as good, but often better, than the standard Gibbs sampler for any particular model. Unfortunately, we cannot directly use the theoretical results to prove that the collapsed version will always converge at a faster rate because neither the augmented version or the collapsed version presented here use strictly Gibbs updates. That is at least some, if not all, of the parameters are updated using Metropolis-within-Gibbs updates because the full conditional distributions are not standard. However, \cite{VanDykJiao2015a} note that MCMC convergence for collapsed samplers using Metropolis updates can still offer improvement but it may not be as good as strictly Gibbs updates.  

The overall goal of this paper is to demonstrate the collapsed approach using the MCMC software in the {\tt R} package \nimble \citep{Valpine:2017vg} to illustrate that PX-DA methods are not necessary and JS models can be fit without data augmentation. The \nimble package allows users to create their own functions which can be compiled and included within an MCMC. Using this capability, as well as, the HMM likelihoods within the {\tt R} package {\tt nimbleEcology} \citep{GoldsteinEtAl2024a}, we analyze 3 real data sets with JS models of varying complexity as examples of practical JS fitting that can readily be used by practitioners just as easily as PX-DA methods. 

%Here are some relevant references the paper will need later:
%
%\cite{Hooten:2023aa} 
%\cite{hooten2024geostatistical}
%\cite{hooten2018prior}
%\cite{schwarz1996general}
%\cite{Royle:2007kx}
%\cite{link2013cautionary}
%\cite{schofield2023estimating}
%\cite{schofield201650}
%\cite{link2005modeling}
%\cite{johnson2010model}
%\cite{king2015capture}
%\cite{mcclintock2020uncovering}
%\cite{glennie2019open}
%\cite{zhang2023flexible}
%\cite{yackulic2020need}

\section{Jolly-Seber Family Models}

Here we describe the class of Jolly-Seber family models as we will use them to develop the collapsed Gibbs sampler methods. All of the variations we consider here have been previously described in the literature, so there are no new model specifications herein, but we are using a general form to include as many different JS aspects for open populations as possible. We begin with the process side of the model and follow up with the observed data portion of JS models.  

In a JS process the population is described as $N$ individuals that are available for capture in at least one capture occasion during the study period. If the study is relatively long-term, this may not be very useful, so, the occasion specific abundance, $N_t$, is often desired instead, where $N_t$ is the size of the population available for capture on occasion $t=1,\dots,K$. Individuals become available and unavailable through an entry and exit process. where individuals can be in 1 of 3 states ``pre-entry" (pre-birth), ``available", and ``post-exit" (post-death). For this paper we expand the set of states so that we may include general multistate models as well. Therefore, an individual can be in one of $J$ states, $s_1,\dots,s_J$ where $s_1$ is the pre-entry state and $s_J$ is the post-exit state. The notation $z_{it} \in \{s_1,\dots,s_J\}$ will represent the true state of individual $i$ on occasion $t$, thus $N_t = \sum_{i=1}^N I(z_{i,t}\ne 1\text{ or } J)$, where $I(\dots)$ is and indicator function. 


One of the major distinguishing features of JS family models relative to other capture-recapture models, such as Cormack-Jolly-Seber (CJS) models, is the entry probability. \citet{schwarz1996general} and \cite{crosbie1985parsimonious} parameterized entry in the JS model using marginal multinomial probabilities $\beta_1,\dots,\beta_K$ which are the unconditional probabilities that an individual enters between occasions $t-1$ and $t$ and is available for capture on occasion $t$. There are many ways to think about entry probabilities but we will base our discussion from this point. The main thing to consider when forming an entry model is that all individuals must enter before occasion $K$, so the probabilities must reflect this constraint. Using the $\beta_t$ entry probabilities we can form the {\it conditional} entry probabilities that are actually used in the JS model computations,
\[
\xi_t = \frac{\beta_t}{\sum_{j=t}^K \beta_j},
\] 
which is the probability that an individual enters between $t-1$ and $t$ given it has not entered prior to $t-1$ but will enter before $K$. Note that we only need to model the $\beta$ up to a normalizing constant to calculate $\xi$. Also, not that the $\sum_j \beta_j = 1$ constraint is reflected in the fact that $\xi_K = 1$. Given a set of $\xi$ conditional probabilities we can obtain the $\beta$ parameters with the relationship,
\[
\beta_t = \left\{ \begin{array}{ll}
\xi_1  & t=1\\
\xi_t \prod_{j=1}^{t-1} (1-\xi_j) & t=2,\dots,K
\end{array} \right. .
\]
Because we are considering multistate models as well, when an individual first enters the available population between $t-1$ and $t$ we must also model which state it enters into. These are denoted,  $\alpha_{t,l}$, where $\sum_{l=2}^{J-1} \alpha_{t,l}=1$. 

We turn our attention to transitions between states (including exit) after entry. Given that and individual has entered prior to occasion $t$, the probability that an individual does not exit the population between occasions $t$ and $t+1$ given it is currently available for capture in state $s_l$ on occasion $t$ is $\phi_{t,l}$, $l=2,\dots,J-1$ and $t=1,\dots,K-1$. That is, the probability that an individual transitions from $s_l \to s_J$ is $1-\phi_{t,l}$. If we consider $s_J$ to be ``death'' then the $\phi$ parameters are the typical survival probabilities. Finally, $\psi_{t,l,l'}$ ($\sum_{l'=2}^{J-1}\psi_{t,l,l'} = 1$) is the probability that an individual transitions from state $s_l$ to $s_{l'}$ between times $t$ and $t+1$ given it was in the available population at time $t$ . 

Now that we have described how individuals transition in and out of availability and through states we can consider a description of the data models for the observed data. Traditionally, data for the JS model are composed of capture histories for each individual observed at least once, $\bx_i = (x_{i1},\dots,x_{iK})$ where $x_{it} \in \{0,1\}$ depending on whether or not individual $i=1,\dots,n$ was captured on occasion $t$. However, we expand this notation in a general way to cover other models such as robust design and multistate JS models. For example, $x_{it}$ might represent the number of times the animal was captured in $R_t$ sub-occasions during primary occasion $t$ which defines a robust design model. Or, $x_{it}$ might be recorded as the individual being in a particular state if it is captured, implying a multistate model \citep{xxx}. We denote the likelihood of observing $x_{i,t}$ given the individual is in state $s_l$ as $[x_{i,t}|z_{i,t}=s_l]$.  Throughout the paper, we use the ``$[A|B]$'' notation to represent a conditional density (distribution) function of $A$ given $B$. We briefly present a few examples for common models. For a standard Jolly-Seber model with a binary capture history and only 1 available state ($s_2$ = ``alive'') the likelihood would be,
\[
[x_{it}|z_{it}=s_2] = \text{Bernoulli}(x_{it}|p_t),
\]
where $p_t$ is the probability of capture on occasion $t$. If the model being fit is a robust design formulation \citep{xxx} where $x_{it}$, are the number of captures in $R_t$ sub-occasions within each main occasion, the likelihood would be,
\[
[x_{it}|z_{it}=s_2] = \text{Binomial}(x_{it}|R_t, p_t). 
\]
For a multistate model where $x_{it}$ is recorded as a categorical state with uncertainty (e.g., \citealt{johnson2016multivariate}) then we use the mixture distribution
\[
[x_{it}|z_{it}=s_l] = (1-p_{t,l})I(x_{it}=0) + p_{t,l}\text{Categorical}(x_{it}|\boldsymbol{\delta}_l,x_{it}\ne0)
\]
where $\boldsymbol{\delta}_l$ is the vector of probabilities for the range of categories that can be observed when the individual is captured in state $s_l$. For example, one might accurately observe the state $x_{it} = s_l$ or it is unknown, say $x_{it} = $``u'', then $\delta_l$ would represent the probability of observing the true state.


\subsection{A brief review of data augmentation for Jolly-Seber models}

To provide a framework for developing the collapsed Gibbs sampler that does not require data augmentation, we provide a brief review of the PX-DA approach for Bayesian JS inference. Let $\bx$ represent the $n\times K$ matrix of observed data, $\bz$ is the associated matrix of true state values. $\bx_{aug} = \mathbf{0}$ and $\bz_{aug}$ are the $M-n \times K$ matrices for the $M-n$ augmented individuals. For the PX-DA approach we combine them, $\bx^+ = (\bx',\bx_{aug}')'$ and $\bz^+ = (\bz',\bz_{aug}')'$, and treat them as single units. The indicator of population inclusion is $\bw^+ = (\mathbf{1}',\bw_{aug}')'$. The first $n$ components are known to be 1 because those individuals are observed (hence available for some occasions). 

The posterior distribution for inference is
\begin{equation}
\label{eq:pxda.post}
[\bt,\psi,\bw_{aug}, \bz^+|\bx^+] \propto [\bx^+|\bz^+,\bw^+,\bt][\bz^+|\bt][\bw^+|\psi][\bt][\psi],
\end{equation}
where $\bt$ are the parameters of the JS model described in the previous section as well as the parameters for the observation models such as capture probabilities which we will discuss in the next section and $\psi$ is the probability that an individual is a member of the superpopulation. The main benefits of the PX-DA approach is that all the distributions on the right-hand side of (\ref{eq:pxda.post}) are all standard distributions, so they can readily be described in the model statements for MCMC software such as {\tt JAGS} and \nimble and the dimension of the products remain fixed at $M$ individuals so trans-dimensional methods are not needed. Gibbs sampling for the PX-DA proceeds by sequentially sampling from the conditional distributions: $[\bz^+|\bt,\bw^+,\bx^+]$, $[\bw_{aug}|\bt,\psi,\bz^+,\bx^+]$,
$[\psi|\bw^+]$, and $[\bt|\bz^+,\bw^+,\bx^+]$. Of course theses may be broken up further into scalar parameter updates. During updates, the raw quantities $\bz^+$ and $\bw^+$, as well as, $\psi$ are not usually useful on their 
own and abundance metric summaries, say $\bvt=f(\bz^+,\bw^+)$, are usually stored such as $N=\bw'\mathbf{1}$ or $N_t=\sum_{i=1}^M z_{it}w_i$. So, the resulting posterior desired is usually, $[\bt,\bvt|\bx^+]$.

\section{Collapsed Gibbs sampling for Jolly-Seber models}

In general a collapsed samplers are constructed by marginalizing over some components of the model, using a standard Gibbs sampler to sample the remaining parameters, then the marginalized parameters are sampled using their posterior predictive distribution given the first sample. For strictly Gibbs samplers faster convergence is achieved because the first sample is not conditioned on the second sample components, reducing the covariance between them and speeding convergence \citep{Liu1994a}. To develop the collapsed JS sampler we will marginalize over all the augmentation components so that we only sample from the conditional distribution,
\[
[\bt,\lambda|\bx] \propto  [\bx|\bt,\lambda][\bt][\lambda],
\]
where $\lambda$ is a parameter that controls the overall superpopulation size. The origin of $\lambda$ will described later, but in essence it takes the place of $\psi$ when not using augmentation.

After drawing a sample from $[\bt,\lambda|\bx]$ we proceed by next drawing a sample from the posterior predictive distribution,
\[
 [\bz, \bz_u, n_u|\bt,\lambda,\bx]
\]
where $n_u$ is the predicted number of unobserved individuals and $\bz_u$ is the associated $n_u \times K$ state matrix. One important aspect to note is that the dimension of $\bz_u$ is not constant over MCMC iterations. Therefore one usually saves the same fixed-dimensional abundance metrics as in the PX-DA scenario, $\bvt = f(\bz,\bz_u,n_u)$. Note the function, $f$ is slightly different than before. For example, $N = n+n_u$ and $N_t = \sum_{i=1}^n z_{it} +  \sum_{j=1}^{n_u} z_{u,jt}$. The resulting posterior of this sampler is
\[
[\bt, \bvt, \lambda|\bx] \propto [\bvt|\bt,\lambda,\bx][\bt,\lambda|\bx]
\]
Notice that if we ignore $\lambda$ as with $\psi$ we obtain a posterior, $[\bt,\bvt|\bx]$, that does not depend on augmented data. 

Now that we have outlined the collapsed Gibbs sampler for JS models we need to examine the details and illustrate that it can be accomplished within the \nimble framework in way that makes it similar in ease-of-use for practitioners relative to the PX-DA approach. The first step is to determine the first sample marginal distribution. 

\subsection{Evaluating marginal posterior of Jolly-Seber models}

In the first step we need to draw a sample from the marginal posterior distribution, $[\bt,\lambda|\bx]$, so let us start with the PX-DA posterior and marginalize over the augmented components. To accomplish this we follow \cite{king2015capture} and \cite{Hooten:2023aa} in partitioning the PX-DA posterior by conditioning on capturing $n$ individuals with capture-histories, $\bx$,
\[
\begin{aligned}
[\bt,\psi,\bw_{aug}, \bz^+|\bx] &\propto [\bx|\bz,\bt,n] [\bz|\bt,n]\\ 
&\qquad \times [\bz_{aug},\bw_{aug}|\bt,\psi,n] \\
&\qquad \times [n|\bt,\psi][\bt][\psi] \\
\end{aligned}
\]
where the probability of observing $n$ individuals during the study is $[n|\bt,\psi] = \text{Binomial}(M, \psi p^*)$ and $p^*$ is the probability of being captured at least once. Thus if we integrate over the augmentation components we obtain
\[
\begin{aligned}
[\bt,\psi, \bz|\bx] &\propto \sum_{\bw_{aug}} \sum_{\bz_{aug}} [\bt,\psi,\bw_{aug}, \bz^+|\bx^+] \\
&=  [\bx|\bz,\bt,n][\bz|\bt,n][n|\bt,\psi][\bt][\psi]
\end{aligned}
\]
Before we continue with marginalizing over $\bz$ let us take a closer look at  $[n|\bt,\psi]$. We do not have any augmented data in the marginal posterior, but we still have the remnants of that process, $M$ and $\psi$. One way to remove their effect is to envision a hypothetical infinite amount of augmented data, that is $M\to \infty$. If we follow this limit while simultaneously allowing $\psi \to 0$ such that $M\psi \to \lambda$, then  $[n|\bt,\psi] \to [n|\bt,\lambda] = \text{Poisson}(\lambda p^*)$.  So, fully marginalizing over the components of an infinitely large augmented data set, yields the posterior,
\[
[\bt,\lambda, \bz|\bx] \propto  [\bx,\bz|\bt,n][n|\bt, \lambda][\bt][\lambda].
\]
This is equivalent in form to the likelihood derived by \cite{glennie2019open} for spatial capture-recapture models when individuals are distributed as a spatial Poisson process.

Now the final step in evaluating the fully marginal posterior distribution, 
\[
[\bt,\lambda|\bx] \propto [\bx|\bt,n][n|\bt,\lambda][\bt][\lambda]
\]
is to marginalize over the partially latent states, $\bz$. \cite{glennie2019open} and \cite{mcclintock2020uncovering} show an individual capture history in a JS models can be formulated as a Hidden Markov Model (HMM). That is, the (unconditional) probability of an individual capture-history can be calculated with the matrix product,
%
\begin{equation}
\label{eq.uncond.lik}
[\bx_i|\bt] = \bpi\bP_1(x_{i1})\bG_1\bP_2(x_{i2}) \cdots \bG_{K-1}\bP_K(x_{iK})\mathbf{1}.
\end{equation}
where $\bpi$ is the initial state probability vector, $\bP_t(x)$ is a diagonal matrix of observation likelihoods for each state, $\bG_t$ is the probability transition matrix for states, and $\mathbf{1}$ is a vector of ones. Therefore, to evaluate the probability of a single capture-history, $\bx_i$, conditional on being captured at least once, we can use the HMM structure and evaluate,
\begin{equation}
\label{eq:dJS}
 [\bx_i|\bt,n]  = \frac{[\bx_i|\bt]}{1-[\bx_i=\mathbf{0}|\bt]}
\end{equation}
where the denominator, $p^*=1-[\bx_i=\mathbf{0}|\bt]$, is the probability of being captured at least once. We can use efficient HMM algorithms to evaluate both the numerator and denominator because the denominator is calculated by simply replacing the observed capture-history with all 0s and evaluating it with (\ref{eq.uncond.lik}) as well.

For the general JS family model the the matrices in the likelihood function (\ref{eq:dJS}) are given as follows. First, he initial state probability vector is 
%
\[
\bpi = \begin{blockarray}{cccccc} 
s_1 & s_2 & \cdots & s_{J-1} & s_J \\
\begin{block}{[cccccc]}
1-\xi_1 & \xi_1\alpha_{1,2} & \cdots & \xi_1\alpha_{1,J-1} & 0 \\
\end{block}
\end{blockarray}.
\]
% 
The observation likelihood matrix, $\mathbf{P}_t(x_{it})$ is a $J\times J$ diagonal matrix with entries, $[x_{it} | s_l]$, $l=1,\dots J$.  Finally, the probability transition matrix is, for $t=1,\dots,K-1$,
%
\begin{equation}
\label{eq.gamma.mat}
\bG_t = \begin{blockarray}{cccccc} 
s_1 & s_2 & \cdots & s_{J-1} & s_J \\
\begin{block}{[ccccc]l}
1-\xi_{t+1} & \xi_{t+1}\alpha_{t+1,2} & \cdots & \xi_{t+1}\alpha_{t+1,J-1} & 0 & s_1\\
0 & \phi_{t,2} \psi_{t,2,2} & \cdots & \phi_{t,2} \psi_{t,2,J-1} & 1-\phi_{t,2} & s_2\\
\vdots &  \vdots & \vdots &\vdots & \vdots & \vdots \\
0 & \phi_{t,J-1} \psi_{t,J-1,2} & \cdots & \phi_{t,J-1} \psi_{t,J-1,J-1} & 1-\phi_{t,J-1} & s_{J-1}\\
0 & 0 & \cdots & 0 & 1 & s_J\\
\end{block}
\end{blockarray}.
\end{equation}
%

Readers should note that we have left the portion of the original JS models dealing with loss upon capture. We find this part is often omitted in practice. If needed, it can be included in the model using a either second exit state in the transition matrix that explicitly feeds to $s_J$ with probability 1 at the next occasion.  

\subsection{Using \nimble to perform collapsed Gibbs sampling}

To perform this approach in \nimble we start by forming distributions for a model description. Unlike \jags the \nimble software possesses the ability to define new distributions and other functions that can be directly incorporated in model statements for MCMC analysis. This capability is accessed through the {\tt nimbleFunction()} function. For categorical data (e.g., multistate capture-histories), the {\tt nimbleEcology} package has already defined an unconditional HMM distribution (\ref{eq.uncond.lik}). This are accessed via the {\tt DHMMo()} HMM distribution. In the {\tt nimbleJSextras} package we used this general HMM distribution to create the bespoke distribution function,
\begin{verbatim}
dJS_ms(x[i,1:K],...,pstar,) <- dHMMo(x[i,1:K],...) / pstar
\end{verbatim}
where {\tt pstar <- 1 - dHMMo(zeros[1:K],...)} and {\tt zeros} is a fixed vector of zeros. The arguments in ``{\tt ...}'' are the $\bpi$, $\bP_t(x_{it})$ and $\bG_t$ matrices as specified by the {\tt nimbleEcology::dHMMo()} function. The the overall probability of capture, {\tt pstar}, has been made a separate argument because it is the same for all individuals and it is unnecessary to recalculate the same quantity for each observed individual. The {\tt nimbleJSextras} package also has a \verb|pstar_ms()| function to calculate this so the user does not have to create the {\tt zeros} vector. 

Now that the \verb|dJS_ms()| distribution and the \verb|pstar_ms()| function are defined, we can it can be used in a \nimble model statement along with the Poisson distribution to model the number of captured individuals, for example,
\begin{verbatim}
for(i in 1:nobs) {
  x[i,1:K] ~ dJS_ms(...,pstar)
}
pstar <- pstar_ms(...)
n ~ dpois(lambda * pstar)
\end{verbatim}
Here, {\tt nobs} = $n$, but we can not pass $n$ as both a constant and a random variable, so we just use a different name for the $i$ index limit. Outside of specifying the HMM matrices for particular JS models, this code is all that is necessary to estimate the model parameters with MCMC. 

Now we turn our focus to sampling the abundance functions $\bvt = f(\bz,\bz_u,n_u)$ in the second stage of the collapsed Gibbs sampler by drawing from 
\begin{equation}
[\bz, \bz_u, n_u|\bt,\lambda,\bx] = [n_u|\bt,\lambda,\bx][\bz_u|n_u,\bt,\bx][\bz|\bt,\bx].
\end{equation} 
To begin, because $[N|\lambda] = \text{Poisson}(\lambda)$ under the same infinite augmentation scheme and $[n,n_u|N,\bt] = \text{Multinomial}(N, p^*, 1-p^*)$ implies 
\[
[n_u|\bt, \lambda] = \text{Poisson}(\lambda (1-p^*))
\]
conditionally independent of the data, $\bx$. So, for each iteration of an MCMC algorithm it can be updated with a draw from a standard Poisson distribution as a posterior predictive variable. Then the prediction for the overall abundance, $N$, results directly from $N = n_u + n$. \cite{schofield2023estimating} and \cite{johnson2010model} both describe abundance estimation in the Poisson approach as {\it predicting} the number of undetected individuals. If one is only interested in the demographic rate parameters, $\bt$, $N$, or derived quantities of these parameters, then one only needs to add 
\begin{verbatim}
nu ~ dpois(lambda*(1-pstar))
Nsuper <- nobs + nu
\end{verbatim}
to the previous \nimble model statement. If quantities such as $N_t$ or population growth are desired, we must sample $\bz$ and $\bz_u$ to derive these within the sampler. 

The final step in the collapsed sampler is to sample the unobserved and partially observed state matrices, $\bz_u$ and $\bz$. Because we have marginalized over the true states, $\bz$ with the HMM-based likelihood calculations it is not as readily apparent how this can be accomplished, however, we can use standard HMM algorithms to efficiently draw from those posterior distributions as well. Then we can extend the method to $\bz_u$.

To begin the final step, we must sample the true states for the observed individuals from the predictive distribution, $[\bz_{i}| \bt, \bx_i]$. This can be accomplished efficiently with the Forward--Filtering Backward--Sampling (FFBS) algorithm for conditional state sampling of HMMs \citep{scott2002bayesian}. The details of the of the FFBS algorithm can be found in Appendix A (algorithm 1). Heuristically, the algorithm works by first using the HMM forward algorithm \citep{zucchini2016hidden} followed by a backward pass through the capture history to sample from $[z_{i,t}|z_{i,t+1},\dots,z_{i,K},\bt, \bx_i]$ at each occasion. To facilitate this sampling algorithm we have coded the FFBS algorithm into the {\tt nimbleJSextras} package and it can be implemented via adding the line,
\begin{verbatim}
for(i in 1:nobs){
  z[i,1:K] <- sample_det_ms(x=x[i,1:K], ...),
  }
\end{verbatim}
this will draw a sample from $[\bz_{i}| \bt, \bx_i]$. Here we have proceeded against convention by specifying a random quantity with a deterministic \nimble function, however, one would never use this distribution as a likelihood, so the coding overhead to specify it as a distribution seems unnecessary and it functions in the same manner as a bespoke distribution.  Once we perform the the FFBS algorithm for each captured individual we can calculate the derived quantity, $n_{t,l} = \sum_{i=1}^n I(z_{i,t}=s_l)$, the abundance of observed individuals in state $s_l$ at time $t$. For simplicity, we use the bold notation, $\bn_t = (n_{t,1},\dots,n_{t,J})$, to represent the vector of state-specific abundances and $n_t = \sum_{l=2}^{J-1} n_{t,l}$ to represent the total abundance of captured individuals available at time $t$. For a standard JS model ($J=3$) this is accomplished with the following code.
\begin{verbatim}
 available[1:nobs,1:K] <- z[1:nobs,1:K]==2
 for(t in 1:K){ n_t[t] <- sum(available[1:nobs, t]) }
\end{verbatim} 

Now that we have the state and time-specific abundances for the observed individuals, we shift to the individuals that are never captured. We begin by drawing the uncaptured superpopulation size $n_u \sim \text{Poisson}(\lambda(1-p_*))$.  We could use the same FFBS algorithm to individually sample the states of all $n_u$ individuals all with the capture-history $\bx_i=\mathbf{0}$. However, Because all uncaptured individuals have the same capture-history, we can use a multinomial version of the FFBS, to sample the state and time-specific abundance in one forward and backward pass (Appendix A, algorithm 2) without simulating individual state histories. This can be accomplished in a \nimble model statement with
\begin{verbatim}
nu_t[1:3, 1:K] <- sample_undet_ms(nu,...)
\end{verbatim}
Again, we acknowledge the unorthodox use of a deterministic function for a random variable. Now that we have posterior samples of $\bn_t$ and $\bn_{u,t}$ we can predict the state and time-specific abundances $\bN_t =  \bn_t + \bn_{u,t}$ and $N_t =  n_t + n_{u,t}$ and any function, $f(\bN_1,\dots,\bN_K)$, such as population growth. Appendix B provides a \nimble code template for fitting JS models via the collapsed sampler. Notably, the sample code illustrates the structure for the data input and HMM matrices contained in the ``{\tt ...}'' arguments of the previous code snippets. 

\section{Examples and Extensions}

\subsection{The Dipper Data}\label{sec:dipper}

The first examination of the ubiquitous European dipper data was presented by \cite{Cormack1964} who used it for demonstration of the Cormack-Jolly-Seber (CJS) model for survival estimation from capture-recapture data. Many years later \cite{Lebreton:1992gr} used the data to demonstrate a JS analysis for abundance estimation. More recently, \cite{Royle:2008kx} demonstrate a Bayesian version of JS modeling using PX-DA. Here we demonstrate analysis of the data using the {\it per capita} recruitment parameterization of \cite{link2005modeling} and \cite{link2005modeling} for modeling entry probability. 

The entry probabilities of the {\it per capita} recruitment model are given by 
\[
\beta_t \propto \left\{ \begin{array}{ll}
1  & t=1\\
d_{t-1} f_{t-1} & t=2,\dots,K
\end{array} \right.
\]
$d_t \propto E[N_t/N|\bt, N]$, the expected relative abundance, and $f_t$ is the {\it per capita} recruitment. The quantity $d_t$ is calculated with the recursion, $d_{t} = d_{t-1}(\phi_{t-1} + f_{t-1})$ where $d_1 \equiv 1$. 

In addition to the {\it per capita} recruitment parameterization of the entry model, we also use a hierarchical approach for estimation of the  parameters. The hierarchical model adds the random effects components,
\begin{center}
$\text{logit } \phi_t \sim N(\mu_\phi,\sigma_\phi^2)$ \\
$\text{logit } p_t \sim N(\mu_p,\sigma_p^2)$, and $\text{log } f_t \sim N(\mu_f,\sigma_f^2)$
\end{center}
This allows shrinkage of the full time model to the temporally constant model we use the complexity prior distribution \citep{xxx} on each of the three the variance components, $\sigma \sim \text{Exponential}(1)$. All mean components of the random effect where given the vaguely informative $N(0,1.5^2)$ prior distribution. 

Code for this example is provided in the package and the dipper data was obtained from the {\tt marked} package \citep{xxx}. In addition, code to fit the fully marginal version of the specification in \cite{Royle:2008kx} is also provided for comparison. 

Parameter estimates  are given in Table \ref{tab:dipper}. The super population size estimate is $N = 312.16$ with SD = 7.35 and 95\% HPD interval [299--336]. Yearly abundance estimates are illustrated in Figure \ref{fig:dipper.Nt}. 


\subsection{Multistate Jolly-Seber Abundance of nesting marine turtles}


\subsection{Bottlenose Dolphin Abundance, Demography, and Heterogeneity}

Using the results of Section 2 we demonstrate the fully marginal analysis of a population with several issues which complicate Jolly-Seber modeling, namely heterogeneity in entry, capture, and survival probabilities. \cite{caruso2024finite} present an analysis of common bottlenose dolphin ({\it Tursiops truncatus}) abundnace in the Tiber River estuary of the Mediterranean Sea using the data augmentation approach. This population has one main aspect which makes abundance analysis using a standard JS model not feasible. There are three populations of individuals: resident, part-time resident, and transient and it is unknown to which population each individual belongs. These three groups have differences in capture probability because part-time and transient animals have times which they are not in the study area, hence, there is a reduced detection probability within capture occasions compared to resident individuals. Moreover, transient animals enter briefly and leave permanently, thus, $\phi$ will be reduced for these individuals compared to resident and part-time resident individuals. Also, recruitment of each population might be different, that is, $\rho_t$ is unequal between latent groups. 

In addition to latent population structure another issue complicates this particular analysis, the time interval between capture occasions is not constant. Therefore, entry and exit probabilities have to be adjusted in the model structure because the intervals over which this process plays out are different. This is straightforward for exit probability, e.g., $\phi_t = \phi^{h_t}$, where $h_t=\tau_{t+1}-\tau_t$ and $\tau_t$ is the time of capture occasion $t$. This will place $\phi$ on a per time unit basis. However, entry probability is more difficult because we are conditioning on entry before the end of the study. Also, with big time gaps there may be a significant probability that an individual can enter the population and exit between occasions. This violates the assumption that an individual cannot enter directly to $s_J$, the exit state. \cite{caruso2024finite} used the per time unit parameterization, as well as, time-specific recruitment probabilities, this adjusts for differing time gaps, however, it does not account for individuals entering and exiting between capture occasions. \cite{caruso2024finite} assume this probability is zero, the standard JS assumption. There are intervals within this data that range between days and months, so this assumption is quite dubious. When occasions are only days apart, individuals can enter and exit within days, however, with a months-long interval individuals cannot exit once they enter. Here we present a multistate robust-design alternative to account for latent structure, as well as, unequal capture intervals.

$n = 195$

$K = 65$ 


\section{Discussion}

Under this distribution $[n_u|\bt,\lambda] =  \text{Poisson}(\lambda (1-p^*))$, which implies $[N=n+n_u|\lambda] =  \text{Poisson}(\lambda)$. If we assign the prior $[\lambda]=\lambda^{-1}$ we obtain the objective scale prior for $N$ \citep{link2013cautionary,schofield2023estimating}.

why go to the trouble to marginalize to undo it with abundance sampling

There are several reasons for this choice. First \cite{schofield201650} and \cite{schofield2023estimating} show there are several beneficial numerical and statistical properties of the Poisson abundance model. In addition, we can motivate the choice from the data augmentation approach. The prior induced on $N$ using data augmentation is  
\[
[N|M, \psi] = \text{binomial}(M,\psi)
\]
where $M$ is a chosen upper bound and $\psi$ is an inclusion probability.  Finally, \cite{link2013cautionary} suggests the scale prior $[N] \propto N^{-1}$ for abundance estimation. One method of approximating this prior is to specify the Poisson prior with Gamma hyper prior as we are using here. Moreover, \cite{schofield2023estimating} shows the posterior distributions are equivalent if one uses the scale priors $[N] \propto N^{-1}$ and $[\lambda] \propto \lambda^{-1}$. Of course, one can always use the improper prior distribution in bespoke MCMC code but the Gamma distribution in already included in software such as NIMBLE and JAGS and can be made as similar as desired, because $\text{Gamma}(\lambda|\epsilon,\epsilon) \to \lambda^{-1}$ as $\epsilon \to 0$. 




\section*{Acknowledgments}
The findings and conclusions in the paper are those of the authors and do not necessarily represent the views of the National Marine Fisheries Service, NOAA. Reference to trade names does not imply endorsement by the National Marine Fisheries Service, NOAA.


\bibliography{bayes_jolly_seber.bib}

\clearpage


\begin{table}
    \caption{Parameter estimates for the random effect recruitment probability analysis of the dipper data.} \bigskip
    \label{tab:dipper}
    \centering
    \begin{tabular}{ccccc}
        \hline\hline
        Parameter & Estimate &  SD & 95\% HPI \\
        \hline
        $\lambda$ & 312.37 & 19.22 & [274.67--349.99]  \\
        $N$ & 312.36 & 7.35 & [299--326]  \\
        \hline
        $\rho_1$ & 0.08 & 0.01 & [0.05 -- 0.10]  \\
         $\rho_2$ & 0.07  & 0.02 &  [0.05 -- 0.11]  \\
         $\rho_3$ & 0.08  & 0.02 &  [0.05 -- 0.11]  \\
         $\rho_4$ & 0.08  & 0.02 &  [0.04 -- 0.11]  \\
         $\rho_5$ & 0.08  & 0.02 &  [0.04 -- 0.11]  \\
         $\rho_6$ & 0.08  & 0.02 &  [0.04 -- 0.11]  \\
         $\rho_7$ & 0.08  & 0.02 &  [0.04 -- 0.11]  \\
         \hline
         $\phi_1$ & 0.57 & 0.06 &  [0.46 -- 0.72]  \\
         $\phi_2$ & 0.52  & 0.05 &  [0.41 -- 0.62]  \\
         $\phi_3$ & 0.52  & 0.05 &  [0.42 -- 0.61]  \\
         $\phi_4$ & 0.59  & 0.05 &  [0.51 -- 0.68]  \\
         $\phi_5$ & 0.58  & 0.04 &  [0.50 -- 0.67]  \\
         $\phi_6$ &  0.57  & 0.05&  [0.49 -- 0.67]  \\
         \hline
         $p_1$ & 0.89 & 0.05 &  [0.80 -- 0.98]  \\
         $p_2$ & 0.88  & 0.05 &  [0.78 -- 0.96]  \\
         $p_3$ &  0.90 & 0.04 &  [0.82 -- 0.97]  \\
         $p_4$ & 0.89  & 0.04 &  [0.82 -- 0.96]  \\
         $p_5$ & 0.89  & 0.04 &  [0.82 -- 0.96]  \\
         $p_6$ & 0.90  & 0.03 & [0.84 -- 0.97]   \\
         $p_7$ &  0.90 & 0.04 &  [0.82 -- 0.97]  \\
         \hline
         $\mu_\rho$ & -2.51   & 0.23  &  [-2.97 -- -2.09]  \\
         $\sigma_\rho $ & 0.09 & 0.08 &  [0.00 -- 0.25]  \\
         $\mu_\phi$     &  0.24 & 0.17 &  [-0.09 -- 0.58]  \\
         $\sigma_\phi $ & 0.25 & 0.20 &  [0.00 -- 0.63]  \\
         $\mu_p$        &  2.17  & 0.37 &  [1.51 -- 2.94]  \\
         $\sigma_p $    & 0.30 & 0.27 &  [0.00 -- 0.81]  \\
         \hline
    \end{tabular}
\end{table}

\clearpage

\begin{figure}
 \includegraphics{dipper_Nt.pdf}
\caption{Time-specific abundance estimates for the dipper data.}
\label{fig:dipper.Nt}
\end{figure}


%\begin{table}
%\caption{\label{tab:notation}Glossary of notation for Bayesian abundance estimation from capture-recapture data}
%\centering
%\begin{tabular}[t]{l>{\raggedright\arraybackslash}p{10cm}}
%\toprule
%Notation & Description\\
%\midrule
%$N$ & Total number of individuals to be in the population at some point during the study\\
%$n$ & Total number of individuals captured at least once during the study\\
%$n_u$ & Number of individuals in the population at some point that were never captured\\
%$i$ & Observed individual index ($i=1,\dots,n$)\\
%$t$ & Capture occasion index ($t=1,\dots,T$)\\
%$c_{it}$ & Capture indicator for individual $i$ on occasion $t$\\
%$\bc_i$ & Capture history for individual $i=1,\dots,n$, $\bc_i = (c_{i1},\dots,c_{iT})$\\
%\midrule
%$\beta_t$ & Probability that an individual enters (or ``born") into the population between occasion $t$ and $t+1$, $t=0,\dots,T-1$, given that it will enter the population at some time during the study\\ 
%$\gamma_t$ & Probability that an individual enters the population between occasions $t$ and $t+1$ given that it will do so sometime during the study and it has not done so before occasion $t$, $t=1,\dots,T-1$\\
%$\delta$ & Probability that an individual is captured at least once during the study\\
%$\phi_t$ & Probability that an animal survives from occasion $t$ to $t+1$; $t=1,\dots,T-1$\\
%$p_t$ & Probability that an individual available for capture, i.e., has entered the population and is currently alive, is captured\\
%\bottomrule
%\end{tabular}
%\end{table}

\clearpage 

\appendix

%\section*{Appendix A: A brief review of data augmentation for abundance estimation}
%
%To this point, we have not discussed estimation of capture-recapture model parameters and abundance, merely reformulated the the POPAN version of the Jolly-Seber model likelihood. Now we can begin discussing Bayesian estimation via Markov chain Monte Carlo (MCMC). MCMC is universally used do to the fact that for most general cases, closed form solutions are not obtainable. In early versions of MCMC software calculation of the unconditional detection probability, $\delta$, was not always straightforward as model complexity increased. To overcome this \cite{Royle:2007kx} introduced the concept of ``data augmentation'' (DA) for abundance estimation. DA uses the power of MCMC to stochastically integrate over all possible states for all individuals in the population. If we were able to know how many unseen individuals existed in the population, as well as, the states for all individuals, we could use the complete data likelihood, which is composed of many simple models that can easily be implemented in MCMC software. The DA method is formulated as follows. First, suppose there are $M>N$ known {\it possible} members of the population. The analyst must choose a large enough $M$ such that is known to be much larger than $N$. Then $M-n$ null individuals are added to the observed capture-histories to to form the complete capture histories, $\tilde{c}$. The DA posterior distribution is given by,    
%\[
%[\bphi, \bp, \ba, \mathbf{w}, \bz, \psi| \tilde{\bc}, M] = \prod_{i=1}^M [w_i|\psi] \prod_{t=1}^T[c_{it}|w_i, z_{it},p_t][z_{it}|\bz_{i,1:(t-1)},\phi_{t-1},\xi_t]
%\]
%where $\tilde{\bc}$ represents the capture-histories for all $M$ individuals in the augmented population and $z_{it}$ is an indicator that individual $i$ is in $\mathcal{A}_t$, $w_i$ is an indicator that animal $i$ belongs to the actual population. note that for observed individuals $w_i = 1$ for augmented individuals it must be estimated. The models for each component are
%\[
%[w_i|\psi] = \text{Bernoulli}(\psi),
%\]
%\[
%[c_{it}|w_i,z_{it},p_t] = \text{Bernoulli}(w_iz_{it}p_t),
%\]
%and
%\[
%[z_{i1}|\xi_1 = \alpha_0] = \text{Bernoulli}(\xi_1),
%\]
%\[
%[z_{it}|\bz_{i,1:(t-1)},\phi_{t-1},\xi_t] = \text{Bernoulli}\left(\phi_{t-1}z_{i,t-1} + \xi_t\prod_{k=1}^{t-1}(1-z_{ik})\right),\quad t=2,\dots,T.
%\]
%Using the DA approach, $N$ is not a parameter, but a derived calculation $N = \sum_{i=1}^M w_i$ and abundance at each occasion is $N_t = \sum_{i=1}^M w_i z_{it}$ \citep{Royle:2008kx}.


\section{Forward-Backward Posterior Abundance Sampling Algorithms}

Here we present two versions of the Forward--Filtering Backward--Sampling (FFBS) algorithm for sampling the conditional posterior distribution of the true state of an individual(s). These algorithms are used within each round of MCMC updates to draw from the state distributions conditional on the observed data and the parameters, $\bt$ at the current iteration. The first version is for updating the true state, $\bz_i=(z_{i,1},\dots,z_{i,K})$ of an observed individual with capture history $\bx_i\ne\mathbf{0}$. The second version is for updating the state and time-specific abundance of all unobserved individuals given the observed data, $\bx=\{\bx_1,\dots,\bx_n\}$, the superpopulation of unobserved individuals, $n_u$, and the current parameters, $\bt$. Both of these algorithms are coded as \nimble functions ({\tt \verb|sample_det_**|} and {\tt \verb|sample_undet_**|}) in the {\tt nimbleJSextras} package available at \href{https://github.com/dsjohnson/nimbleJSextras}{https://github.com/dsjohnson/nimbleJSextras} and their use is illustrated in the code for each of the example analyses/ 

\begin{algorithm}[H]
\caption{FFBS for Observed Individuals}
\begin{algorithmic}[1]
\STATE \textbf{Inputs:} $\bx_i$, $\bpi$, $\{\bG_t\}$, $\{\mathbf{P}_t(x_{i,t})\}$
\STATE \textbf{Output:} posterior draw of hidden state sequence $\mathbf{z}_i \sim [\mathbf{z}_i|\bx_i, \bt]$

\vspace{0.5em}
\STATE \textbf{Forward pass:}
\STATE Compute $\mathbf{f}_1 = \bpi \mathbf{P}_1(x_{i,1})$
\STATE Normalize $\mathbf{f}_{1} = \mathbf{f}_{1}/\text{sum}(\mathbf{f}_{1})$
\FOR{$t = 1$ to $K-1$}
    \STATE $\mathbf{f}_{t+1} = \mathbf{f}_{t} \bG_{t} \mathbf{P}_{t+1}(x_{i,t+1}) $
    \STATE Normalize $\mathbf{f}_{t+1} = \mathbf{f}_{t+1}/\text{sum}(\mathbf{f}_{t+1})$
\ENDFOR

\vspace{0.5em}
\STATE \textbf{Backward sampling:}
\STATE Sample $z_{i,K} \sim \text{categorical}(\mathbf{f}_K)$
\FOR{$t = K-1$ down to $1$}
    \STATE Compute $\mathbf{b} = \mathbf{f}_t \odot \bG_t[,z_{i,t+1}]$ ($\odot$ \text{ is elementwise multiplication})
    \STATE Normalize $\mathbf{b} = \mathbf{b}/\text{sum}(\mathbf{b})$
     \STATE Sample $z_{i,t} \sim \text{categorical}(\mathbf{b})$
\ENDFOR
\STATE Return $\mathbf{z}_i=(z_{i,1},\dots,z_{i,K})$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{FFBS for Unobserved Individuals}
\begin{algorithmic}[1]
\STATE \textbf{Inputs:} $\bpi$, $\{\bG_t\}$, $\{\mathbf{P}_t(0)\}$, $n_u$.
\STATE \textbf{Output:} posterior draw of abundance of unobserved individuals $\mathbf{n}_{u} \sim [\mathbf{n}_{u}|\bx, n, \bt]$, where $\mathbf{n}_u$ is a $J\times K$ matrix of state- and occasion-specific abundance. 

\vspace{0.5em}
\STATE \textbf{Forward pass:}
\STATE Compute $\mathbf{f}_1 = \bpi \mathbf{P}(0)$
\STATE Normalize $\mathbf{f}_{1} = \mathbf{f}_{1}/\text{sum}(\mathbf{f}_{1})$
\FOR{$t = 1$ to $K-1$}
    \STATE $\mathbf{f}_{t+1} = \mathbf{f}_{t} \bG_{t} \mathbf{P}_{t+1}(0) $
    \STATE Normalize $\mathbf{f}_{t+1} = \mathbf{f}_{t+1}/\text{sum}(\mathbf{f}_{t+1})$
\ENDFOR
\vspace{0.5em}
\STATE \textbf{Backward sampling:}
\STATE Sample $\mathbf{n}_u[,K] \sim \text{multinomial}(n_u,\mathbf{f}_K)$
\FOR{$t = K-1$ down to $1$}
    \FOR{$l=1$ to $J$}
     \STATE Compute $\mathbf{b} = \mathbf{f}_t \odot \bG_t[,l]$ ($\odot$ \text{ is elementwise multiplication})
     \STATE Normalize $\mathbf{b} = \mathbf{b}/\text{sum}(\mathbf{b})$
     \STATE Sample $\mathbf{Z}[,l] \sim \text{multinomial}(\mathbf{n}_u[l,t+1], \mathbf{b})$
    \ENDFOR
    \FOR{$l=1$ to $J$}
        \STATE $\mathbf{n}_u[l,t] = \text{sum}(\mathbf{Z}[l,])$
    \ENDFOR
\ENDFOR
\STATE Return $\mathbf{\mathbf{n}_u}$
\end{algorithmic}
\end{algorithm}

\clearpage

\section{{\tt nimble} Code Template for Fitting JS Models}

\subsection{Model definition}

See {\tt ?nimbleEcology::dDHMMo} in {\tt R} for a more detailed explanation of the HMM arguments, {\tt inits} (contains $\bpi$), {\tt probObs} (contains $\bP_t(x_{it})$ values), and {\tt probTrans} (contains $\bG_t$ values). The following code would be contained in a file, say, \verb|js_model_code.R|:

\small 
\begin{verbatim}
require(nimble)
require(nimbleEcology)
require(nimbleJSextras)

js_model_def <- nimbleCode({

  ######################################################
  ### Model parameter and HMM argument specification ###
  ######################################################
  #---------------------------------------------------------------------------
  # Capture probabilities
  # ---------------------------------------------------------------------------
  for(t in 1:K){
    p[t] ~ dunif(0,1)
  }
  #---------------------------------------------------------------------------
  # Detection matrix values, P_t(x), J x O x K (O = number of observable states)
  #---------------------------------------------------------------------------
  for(t in 1:K){
    Pmats[1,1,t] <- 1
    Pmats[1,2,t] <- 0
    Pmats[2,1,t] <- 1-p[t]
    Pmats[2,2,t] <- p[t]
    Pmats[3,1,t] <- 1
    Pmats[3,2,t] <- 0
  }
  #-----------------------------------------------------------------------------
  # Individual recruitment probabilities
  #-----------------------------------------------------------------------------
  for(t in 1:K){
    rho[t] ~ dunif(0,1)
  }
  # Conditional entry (at or before K)
  for(t in 1:(K-1)){
    xi[t] <- rho[t]/(1-prod(1-rho[t:K]))
  }
  xi[K] <- 1 # for exactness
  #---------------------------------------------------------------------------
  # Initial entry probability
  #---------------------------------------------------------------------------
  pi[1] <- 1-xi[1]
  pi[2] <- xi[1]
  pi[3] <- 0
  #---------------------------------------------------------------------------
  # Transition probability matrix (J x J x K)
  #---------------------------------------------------------------------------
  for(t in 1:(K-1)){
    Gamma[1,1,t] <- 1-xi[t]
    Gamma[1,2,t] <- xi[t]
    Gamma[1,3,t] <- 0
    Gamma[2,1,t] <- 0
    Gamma[2,2,t] <- phi[t]
    Gamma[2,3,t] <- 1 - phi[t]
    Gamma[3,1,t] <- 0
    Gamma[3,2,t] <- 0
    Gamma[3,3,t] <- 1
    # exit probability
    phi[t] ~ dunif(0,1)
  }
  ####################################
  ### Jolly-Seber model components ###
  ####################################
  #---------------------------------------------------------------------------
  # Unconditional detection probability
  #---------------------------------------------------------------------------
  pstar <- pstar_ms(
    init = pi[1:3], probObs = Pmats[1:3, 1:2, 1:K], 
    probTrans = Gamma[1:3, 1:3, 1:(K-1)],
    len=K)
  #---------------------------------------------------------------------------
  # JS likelihood for observed individuals
  #---------------------------------------------------------------------------
  for(i in 1:nobs){
    x[i, 1:K] ~ dJS_ms(
      init = pi[1:3], probObs = pmat[1:3, 1:2, 1:K], 
      probTrans = Gamma[1:3, 1:3, 1:(K-1)],
      pstar = pstar, len = K
    )
  }
  n ~ dpois(lambda*pstar)

  # lambda prior distribution approximates objective scale prior for N
  lambda ~ dgamma(1.0e-6, 1.0e-6)
  ##############################################################
  ### Posterior predictive sampling for abundance quantities ### 
  ##############################################################
  nu ~ dpois(lambda*(1-pstar))
  Nsuper <- n+nu

  nu_t[1:3, 1:K] <- sample_undet_ms(n=nu, init=pi[1:3], probObs=pmat[1:3,1:2,1:K],
                                     probTrans=Gamma[1:3, 1:3, 1:(K-1)], len=K)
  for(i in 1:nobs){
    z[i,1:K] <- sample_det_ms(x=x[i,1:K], init=pi[1:3],
                                    probObs=pmat[1:3, 1:2, 1:K],
                                    probTrans=Gamma[1:3, 1:3, 1:(K-1)])
  }
  available[1:nobs, 1:K] <- z[1:nobs,1:K]==2
  for(t in 1:K){
    Nd[t] <- sum(available[1:nobs, t])
    N[t] <- Nd[t] + nu_t[2,t]
  }
})
\end{verbatim}
\normalsize

\subsection{Analysis Code}

Here the matrix {\tt x} are the capture histories coded as 1 = not captured and 2 = captured, instead of 0/1.
\small
\begin{verbatim}
#-----------------------------------------------------------------------------
# Load and compile model code
# -----------------------------------------------------------------------------
source("js_model_code.R")

js_model <- nimbleModel(
  code = js_model_def,
  constants = list(K=ncol(x), nobs=nrow(x)),
  data = list(x = x, n = nrow(x)),
  inits = list(
    p = rep(0.5, K),
    phi = rep(0.7, K-1),
    rho = rep(0.1, K),
    lambda = 2*nrow(x)
    )
)
c_js_model <- compileNimble(js_model)
js_mcmc <- buildMCMC(
  js_model, 
  monitors=c("p","rho", "phi","lambda","Nsuper", "N") 
  )
c_js_mcmc <- compileNimble(js_mcmc)
#-----------------------------------------------------------------------------
# Run MCMC
#-----------------------------------------------------------------------------
set.seed(8675309)
samples <- runMCMC(c_js_mcmc, niter = 25000, nburnin = 5000, nchains = 1,
                   thin = 1, samplesAsCodaMCMC = TRUE, progress = TRUE)

\end{verbatim}
\normalsize
% \section{Transition matrices for unequal capture occasions}

% Here we develop analytical solutions for accounting for unequal capture intervals for standard JS models or models where groups membership is unknown, such as in the dolphin example. If we have other states then solutions become quite cumbersome and we suggest the best approach is to augment occasions where $p_{t,l}=0$ for all states. If the occasions times are not integer based for some time unit, we suggest a continuous-time model (see \citealt{xxx}) for multistate situations. However, if a group-based JS model is desired, there are analytic solutions as illustrated below.  

% \subsection{Matrices based on continuous-time models}

% When we have uneven sampling intervals, the obvious choice is to model the entry and exit process in continuous time. A continuous-time Markov chain model for a JS entry-exit process is defined by the rate matrix 
% \[
% \mathbf{Q} = \left[
% \begin{array}{ccc}
% -b & b & 0 \\ 
% 0 & -d & d \\
% 0 & 0 & 0
% \end{array}
% \right],
% \]
% where $b$ is the rate of ``birth'' (entry) and $d$ is the rate of ``death'' (exit). The probability transition matrix for the JS-HMM formulation for a time gap of $h$ units is
% \[
% \bG = \text{expm}(\mathbf{Q}h),
% \]
% where expm() is the matrix exponential function. Unfortunately the expm() function is not readily available in MCMC software such as NIMBLE, however, for the basic JS model with 3 states there is an analytical solution. If we combine this with occasion specific transition rates, $b_t$ and $d_t$, and the fact that we need to condition on entry before the end of the study, we get the following probability that an individual has not recruited by occasion $t+1$ given it has not recruited by occasion $t$,
% \[
% \Gamma_{t,1,1} = \frac{\exp(-b_th_t) - \exp(-\sum_{s=t}^{K-1} b_s h_s)}{1-\exp(-\sum_{s=t}^{K-1} b_s h_s)}
% \]
% This is derived from the fact that in a CTMC wait times between transitions are exponentially distributed with rates equal to the diagonal of $\mathbf{Q}$ and the exponential distribution is also memoryless. The memoryless property implies that given the individual has not recruited by a capture occassion, the exponential distribution resets with the (possibly) new rate. Using these same results we can obtain the probability that an individual has been recruited but not exited between occasions
% \[
% \begin{aligned}
% \Gamma_{t,1,2} &= \frac{\int_0^{h_t} b_t\exp(-bu)\exp(-d(h_t-u))du}{1-\exp(-\sum_{s=t}^{K-1} b_s h_s)} \\
% &= \frac{}{1-\exp(-\sum_{s=t}^{K-1} b_s h_s)}
% \end{aligned}
% \]

% \subsection{Matrices for integer based capture occasions}

% For integer-based capture occasions we assume that the unequal intervals are based on failure to perform capture sessions at certain times resulting in integer valued $h_t = \tau_{t+1}-\tau_t$. The Markov process for the true states of the individuals at all times are governed by the base transition probability matrices,
% \[
% \tilde{\bG}_{t(s)} = \left[
% \begin{array}{ccc}
% 1-\gamma_{t(s)} & \gamma_{t(s)} & 0 \\
% 0 & \phi_{t(s)} & 1-\phi_{t(s)} \\
% 0 & 0 & 1
% \end{array} 
% \right]
% \]
% where $s$ refers to the evenly spaced time unit intervals between occasions. Thus, what we need for the HMM likelihood are
% \[
% \bG_t = \prod_{s=1}^{h_t} \bG_{t(s)}
% \]
% conditioned on the fact that entry happens before $\tau_K$. We will make the simplifying assumption that $\gamma_{t(s)} = \gamma_t$ and $\phi_{t(s)} = \phi_t$. To determine the transition probabilities over $h_t$ missed occasions we only need to closely examine the first row. Once an individual transitions from $s_1 \to s_2$ the probability of $s_2 \to s_2$, i.e., not exiting is just $\phi_t^{h_t}$ and the probability of $s_2 \to s_3$, exiting the population is $1-\phi_t^{h_t}$ as is classically done in Cormack-Jolly-Seber (CJS) models for survival estimation. So, let us examine the transitions from $s_1$. For $s_1 \to s_1$ these are given by
% \[
% \Gamma_{t,1,1} = \frac{ (1-\gamma_t)^{h_t} - \prod_{u=t}^{K-1}(1-\gamma_t)^{h_u}}{1 - \prod_{u=t}^{K-1}(1-\gamma_u)^{h_u}}
% \]
% This is just the probability that entry happens after $\tau_t$ but before $\tau_K$ given it happens between $\tau_t$ and $\tau_K$. For $s_1 \to s_2$ we get
% \[
% \begin{aligned}
% \Gamma_{t,1,2} &= \frac{\sum_{u=0}^{h_t-1} (1-\gamma_t)^u \gamma_t \phi^{h_t-u-1} }{1 - \prod_{u=t}^{K-1}(1-\gamma_u)^{h_u}} \\
% &= \frac{\gamma_t \phi^{h_t-1} \sum_{u=0}^{h_t-1} \left(\frac{1-\gamma_t}{\phi}\right)^u }{1 - \prod_{u=t}^{K-1}(1-\gamma_u)^{h_u}} \\
% &= \frac{\gamma_t \phi^{h_t-1} \cdot \frac{1-\left(\frac{1-\gamma_t}{\phi_t}\right)^{h_t}}{1-\left(\frac{1-\gamma_t}{\phi_t}\right)}}{1 - \prod_{u=t}^{K-1}(1-\gamma_u)^{h_u}}
% \end{aligned}
% \]
% The last equality results because the sum is a geometric series with closed form solution. 

%\section{Transition matrices for unequal capture occasions}
%
%Here we develop analytical solutions for accounting for unequal capture intervals for standard JS models or models where groups membership is unknown, such as in the dolphin example. If we have other states then solutions become quite cumbersome and we suggest the best approach is to augment occasions where $p_{t,l}=0$ for all states. If the occasions times are not integer based for some time unit, we suggest a continuous-time model (see \citealt{xxx}) for multistate situations. However, if a group-based JS model is desired, there are analytic solutions as illustrated below.  
%
%\subsection{Matrices based on continuous-time models}
%
%When we have uneven sampling intervals, the obvious choice is to model the entry and exit process in continuous time. A continuous-time Markov chain model for a JS entry-exit process is defined by the rate matrix 
%\[
%\mathbf{Q} = \left[
%\begin{array}{ccc}
%-b & b & 0 \\ 
%0 & -d & d \\
%0 & 0 & 0
%\end{array}
%\right],
%\]
%where $b$ is the rate of ``birth'' (entry) and $d$ is the rate of ``death'' (exit). The probability transition matrix for the JS-HMM formulation for a time gap of $h$ units is
%\[
%\bG = \text{expm}(\mathbf{Q}h),
%\]
%where expm() is the matrix exponential function. Unfortunately the expm() function is not readily available in MCMC software such as NIMBLE, however, for the basic JS model with 3 states there is an analytical solution. If we combine this with occasion specific transition rates, $b_t$ and $d_t$, and the fact that we need to condition on entry before the end of the study, we get the following probability that an individual has not recruited by occasion $t+1$ given it has not recruited by occasion $t$,
%\[
%\Gamma_{t,1,1} = \frac{\exp(-b_th_t) - \exp(-\sum_{s=t}^{K-1} b_s h_s)}{1-\exp(-\sum_{s=t}^{K-1} b_s h_s)}
%\]
%This is derived from the fact that in a CTMC wait times between transitions are exponentially distributed with rates equal to the diagonal of $\mathbf{Q}$ and the exponential distribution is also memoryless. The memoryless property implies that given the individual has not recruited by a capture occassion, the exponential distribution resets with the (possibly) new rate. Using these same results we can obtain the probability that an individual has been recruited but not exited between occasions
%\[
%\begin{aligned}
%\Gamma_{t,1,2} &= \frac{\int_0^{h_t} b_t\exp(-bu)\exp(-d(h_t-u))du}{1-\exp(-\sum_{s=t}^{K-1} b_s h_s)} \\
%&= \frac{}{1-\exp(-\sum_{s=t}^{K-1} b_s h_s)}
%\end{aligned}
%\]
%
%\subsection{Matrices for integer based capture occasions}
%
%For integer-based capture occasions we assume that the unequal intervals are based on failure to perform capture sessions at certain times resulting in integer valued $h_t = \tau_{t+1}-\tau_t$. The Markov process for the true states of the individuals at all times are governed by the base transition probability matrices,
%\[
%\tilde{\bG}_{t(s)} = \left[
%\begin{array}{ccc}
%1-\gamma_{t(s)} & \gamma_{t(s)} & 0 \\
%0 & \phi_{t(s)} & 1-\phi_{t(s)} \\
%0 & 0 & 1
%\end{array} 
%\right]
%\]
%where $s$ refers to the evenly spaced time unit intervals between occasions. Thus, what we need for the HMM likelihood are
%\[
%\bG_t = \prod_{s=1}^{h_t} \bG_{t(s)}
%\]
%conditioned on the fact that entry happens before $\tau_K$. We will make the simplifying assumption that $\gamma_{t(s)} = \gamma_t$ and $\phi_{t(s)} = \phi_t$. To determine the transition probabilities over $h_t$ missed occasions we only need to closely examine the first row. Once an individual transitions from $s_1 \to s_2$ the probability of $s_2 \to s_2$, i.e., not exiting is just $\phi_t^{h_t}$ and the probability of $s_2 \to s_3$, exiting the population is $1-\phi_t^{h_t}$ as is classically done in Cormack-Jolly-Seber (CJS) models for survival estimation. So, let us examine the transitions from $s_1$. For $s_1 \to s_1$ these are given by
%\[
%\Gamma_{t,1,1} = \frac{ (1-\gamma_t)^{h_t} - \prod_{u=t}^{K-1}(1-\gamma_t)^{h_u}}{1 - \prod_{u=t}^{K-1}(1-\gamma_u)^{h_u}}
%\]
%This is just the probability that entry happens after $\tau_t$ but before $\tau_K$ given it happens between $\tau_t$ and $\tau_K$. For $s_1 \to s_2$ we get
%\[
%\begin{aligned}
%\Gamma_{t,1,2} &= \frac{\sum_{u=0}^{h_t-1} (1-\gamma_t)^u \gamma_t \phi^{h_t-u-1} }{1 - \prod_{u=t}^{K-1}(1-\gamma_u)^{h_u}} \\
%&= \frac{\gamma_t \phi^{h_t-1} \sum_{u=0}^{h_t-1} \left(\frac{1-\gamma_t}{\phi}\right)^u }{1 - \prod_{u=t}^{K-1}(1-\gamma_u)^{h_u}} \\
%&= \frac{\gamma_t \phi^{h_t-1} \cdot \frac{1-\left(\frac{1-\gamma_t}{\phi_t}\right)^{h_t}}{1-\left(\frac{1-\gamma_t}{\phi_t}\right)}}{1 - \prod_{u=t}^{K-1}(1-\gamma_u)^{h_u}}
%\end{aligned}
%\]
%The last equality results because the sum is a geometric series with closed form solution. 


\end{document}

